This is our simulator for evaluating the impact of errors in
estimating the size when performing size-based scheduling in big-data
workloads. Details in our technical report, available at XXX.

Needed software:
 - wget (to get the datasets)
 - Python 3 (should also work -- untested -- with Py2.7)
 - Python libraries: numpy, matplotlib (for plots)

I got reported that this appears not to be sufficient with the Py2.7
installed by default on MacOS.

=== GET THE WORKLOADS ===

$./get_datasets
(will get the workloads from the SWIM git repository)

=== RUN THE EXPERIMENT ===

usage: experiment.py -h

This will compute the sojourn times for each of the jobs in
filename.tsv using FIFO, PS, SRPT and two variants of FSP that cope
with errors (described again in our technical report). When performing
experiments, it will print the mean sojourn times for each instance of
the experiments. Since FIFO and PS do not suffer from estimation
errors and they only depend on the trace, they will be computed only
once rather than repeated depending on the number of iterations.

Results will be output in a binary file (Python's shelve format) named
'results_filename[.tsv stripped]_sigma_d-over-n_load.s'. If such a
file already exists, we simply add the results of more experiment runs
to it.

=== PLOT THE RESULTS ===

usage: plot_sojourn_vs_error.py -h
usage: plot_sojourn_vs_load.py -h
usage: plot_sojourn_vs_dn.py -h

Exactly as above, but varying d-over-n and keeping load fixed. Output
graphs for the "paper" option will be
sojourn-vs-dn_dataset_sigma_load.pdf.

=== REPEAT THE EXPERIMENTS IN THE TECHNICAL REPORTS ===

$./do_experiments
$./do_plots